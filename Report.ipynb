{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Learning Project : sequential weighted scheme comparison\n",
    "_Supervisor : Gilles Stoltz (http://stoltz.perso.math.cnrs.fr/enseignements.html)_\n",
    "\n",
    "_Author : Hugo Vallet (https://uk.linkedin.com/in/hugovallet)_\n",
    "\n",
    "_Course : Learning and sequential optimization, Data Science track, Paris-Saclay University_\n",
    "\n",
    "## Intoduction\n",
    "In this report, we will test multiple weighting schemes for sequential learning problems. As a reminder, the sequential learning setup is the following :\n",
    "- We have a set of $N$ \"experts\", or predictors, $N \\in \\mathbb{N}$\n",
    "- We get the data \"online\", meaning that we do not have access directly to a batch : the examples (data points in $\\mathbb{R}^d$) are coming iteratively in time.\n",
    "- At each step $t$, for a given data point $x_t$, the prediction is a weighted sum of the individual predictions of the  experts. \n",
    "\n",
    "The main challenge here is to find the best choice of weights at a time t, given the data (and the associated errors of the experts) seen in the past.\n",
    "\n",
    "## Dataset\n",
    "Here, I consider first a toy regression problem generated using the library Sklearn. This dataset is composed of 10000 iid obeservations in $\\mathbb{R}^{15}$ following a Gaussian distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Line magic function `%matplolib` not found.\n"
     ]
    }
   ],
   "source": [
    "#Import libraries\n",
    "%matplolib inline\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "py.sign_in('hugovallet', '1po3tzu1j2')\n",
    "\n",
    "#Generate the dataset\n",
    "data,y = make_regression(n_samples = 10000, n_features = 15, n_informative = 15, n_targets = 1 , bias = 0, noise=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of experts and methodology\n",
    "The aim of the report is to study the behaviour of the Ridge-weighting and EWA algorithms on a given set of predictors for an incoming flow of data. Hence, we are not really interested in optimizing the choice of experts. Finally, these experts are fixed at the beginning of the problem : they are not intrinsectly modified during the online process, only their weights are updated.\n",
    "\n",
    "With this setup, I created a set of random trees that I will use as experts. The trees are generated using a random-forest training approach :\n",
    "\n",
    "For each tree :\n",
    "- Select randomly a set of features on which the tree will be trained\n",
    "- Construct the tree splitting iteratively on a random subset of this features. The split is chosen with a entropy-based criterion : the chosen split is the one maximizing the \"information\" gain.\n",
    "\n",
    "I use half of my data for this initial training of the experts and I keep the remaining half for the study of the online weighting algorithms (Ridge and EWA). This approach seems valid in the sens that :\n",
    "- The datapoints follow the same distribution. Thus, theorically, the experts learnt during the first phase are relevant for the sequential problem.\n",
    "- Because of the random forest procedure (decribed above) the trees generated are really of variable efficiency. Some of them, which are based on very informative features, perform a lot better than others. Thus, finding a good way to average their predictions (and not taking just the basic average, as used in classical random forest prediction) is relevant.\n",
    "\n",
    "### First phase : training of the experts (generating the trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R-squared score :  0.465491661664\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=42)\n",
    "reg = RandomForestRegressor(n_estimators=50, max_features='sqrt')\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print \"Adjusted R-squared score : \",r2_score(y_pred,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using random forests, it is always interesting to check the features which contribute the most is the model. Sklearn implementation proposes a keys-in-hand method to compute an individual score of the different features which is directly linked to the number of times the features where selected for a split (i.e. how informative they are)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/6.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = reg.feature_importances_\n",
    "trace = go.Bar(\n",
    "         x=[(\"Feature\"+str(i)) for i in range(len(feature_importances))],\n",
    "         y=feature_importances\n",
    ")\n",
    "trace = [trace]\n",
    "py.iplot(trace, filename='importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, it is easy to intuit that the trees which are not based on the features 5 and 14, for instance, will be less efficient than the other ones. To check that, let's plot the individual errors of our trees in the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "trees = reg.estimators_\n",
    "errors = []\n",
    "for tree in trees :\n",
    "    error = mean_squared_error(y_test,tree.predict(X_test))\n",
    "    errors.append(error)\n",
    "    \n",
    "plt.figure()\n",
    "plt.bar(np.arange(len(errors)),errors)\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the real value of series to predict and the prediction made by a uniform average of the predictions of the trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/8.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1 = go.Scatter(x = np.arange(len(y_test)),y = y_test,name = \"Real value\")\n",
    "trace2 = go.Scatter(x = np.arange(len(y_pred)),y = y_pred, name = \"Predicted value (uniform average of predictors)\")\n",
    "trace = [trace1, trace2]\n",
    "py.iplot(trace, filename='prediction VS real value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the sequential algorithms\n",
    "\n",
    "Here are implemented to classical sequential agolrithms : EWA and Riddge.\n",
    "\n",
    "### EWA (Exponentially Weighted Average)\n",
    "\n",
    "In the EWA algorithm, the weights are updated respecting the following law :\n",
    "\n",
    "$$ \\forall j\\in[1,N], w_{1,j} = \\frac{1}{N} $$\n",
    "\n",
    "$$ \\forall(t,j)\\in [2,T]\\times[1,N],  w_{t,j} = \\frac{1}{W}exp(-\\eta\\sum\\limits_{s=1}^{t-1} l_{s,j}) $$\n",
    "\n",
    "whith $W$ a normalization factor (to make the weights sum to 1) and $l_{s,j}$ the error (loss) observed for the predictor $j$ at the step $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_EWA_weights(eta,weights_matrix,loss_matrix):\n",
    "    \"\"\"\n",
    "    This method takes a weights and loss matrices as input (size t,N where t is the current number of iterations) \n",
    "    and returns the new row as well as the updated weights matrix for further computations.\n",
    "    \"\"\"\n",
    "    weights = np.exp(-eta*np.sum(loss_matrix,axis=0))\n",
    "    W = np.sum(weights)\n",
    "    weights = np.expand_dims(weights/W,axis=0)\n",
    "    \n",
    "    return weights\n",
    "    \n",
    "def compute_losses(experts, X, y, loss = \"least squares\"):\n",
    "    \"\"\"\n",
    "    This methods takes as input the loss function we want to use to measure the distance between the predictions of \n",
    "    the experts, for a given example X (vector of observations), and the real value y.\n",
    "    The experts here are all sklearn regressors objects stocked in an array.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    for expert in experts :\n",
    "        pred = expert.predict(X)[0]\n",
    "        if loss == \"least squares\" :\n",
    "            obs_loss = (y-pred)**2\n",
    "            losses.append(obs_loss)\n",
    "    \n",
    "    \n",
    "    return losses\n",
    "\n",
    "def weighted_prediction(experts,xt,weights):\n",
    "    predictions = []\n",
    "    for expert in experts :\n",
    "        pred = expert.predict(xt)[0]\n",
    "        predictions.append(pred)\n",
    "    predictions = np.array(predictions)\n",
    "    return np.sum(weights*predictions)\n",
    "    \n",
    "def draw_example(X,y):\n",
    "    n_examples = X.shape[0]\n",
    "    index = np.random.randint(low=0,high=n_examples)\n",
    "    Xt = X[index,:].reshape(1, -1)\n",
    "    yt = y[index]\n",
    "    return Xt,yt\n",
    "\n",
    "def compute_regret(weights_matrix,loss_matrix):\n",
    "    \"\"\"\n",
    "    Compute the regret given the 2 matrices of interest.\n",
    "    \"\"\"\n",
    "    #Compute the term on the left of the regret formula for a linear loss function \n",
    "    first_term = sum(sum(weights_matrix*loss_matrix))\n",
    "    second_term = min(np.sum(loss_matrix,axis=0))\n",
    "    return first_term - second_term\n",
    "\n",
    "def compute_bound(N,eta,time,m,M):\n",
    "    return np.log(N)/eta + eta*(M-m)**2/8*time   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we have a batch of data. To simulate the online approach, at each step t in the algorithm, we will draw uniformly an example on that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def EWA(experts, X, y, T, eta):\n",
    "    #Initialization\n",
    "    N = len(experts)\n",
    "    \n",
    "    weights_matrix = np.zeros((T,N))\n",
    "    weights_matrix[0,:] = 1/float(N)*np.ones((1,N))\n",
    "    weights = weights_matrix[0,:]\n",
    "    loss_matrix = np.zeros((T,N))\n",
    "    #error vec 1 stocks the errors made by the aggregated model\n",
    "    error_vec1=[]\n",
    "    #error vec 2 stocks the error made by the random forest (for further performances comparisons)\n",
    "    error_vec2=[]\n",
    "    #The regrets at each step for our algorithm\n",
    "    regrets = []\n",
    "    \n",
    "    #Start sequential learning\n",
    "    for t in range(T):\n",
    "        #Draw an example from the batch\n",
    "        Xt,yt = draw_example(X,y)\n",
    "        #Oberve the losses of the experts\n",
    "        loss_matrix[t,:] = compute_losses(experts,Xt,yt,loss = \"least squares\")\n",
    "        #Make a prediction\n",
    "        y_pred = weighted_prediction(experts,Xt,weights)\n",
    "        #Compute regret    \n",
    "        regret = compute_regret(weights_matrix,loss_matrix)\n",
    "        regrets.append(regret)\n",
    "        #Update weights\n",
    "        if t<T-1:\n",
    "            weights_matrix[t+1,:] = compute_EWA_weights(eta,weights_matrix,loss_matrix)\n",
    "            weights = weights_matrix[t+1,:]\n",
    "            \n",
    "        #Stock errors for plotting\n",
    "        error1 = (yt-y_pred)**2\n",
    "        error_vec1.append(error1)\n",
    "        error2 = (yt-reg.predict(Xt)[0])**2\n",
    "        error_vec2.append(error2)\n",
    "    \n",
    "    return loss_matrix, weights_matrix, error_vec1,error_vec2,regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X_test\n",
    "y = y_test\n",
    "T=100\n",
    "experts = reg.estimators_\n",
    "eta = 0.00000001\n",
    "loss_matrix, weights_matrix, error_vec1,error_vec2,regrets = EWA(experts, X, y, T, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_weights_3d(weights_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the high number of experts, this plot is a bit messy, here is another visualization of the evolution of weights accross time :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_heatmap(weights_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is clear that choosing a good $\\eta$ is foundamental : a too large value will make the update rules too sensitive and thus block the weights on few experts, a too small value will restrain the algorithm's ability to adapt to the incoming flow of data and the prediction will be similar to a classical RF.\n",
    "\n",
    "In the previous example I cheated : because I have a batch of data, I computed a first time the loss matrix to have an idea of $m$ and $M$ so that i could take an optimal fixed $\\eta$.\n",
    "\n",
    "### Variable step EWA\n",
    "\n",
    "To have a better control on the regret, a classical approach is to make $\\eta$ vary accross time. \n",
    "One possible implementation is to make the $\\eta$ depending on the distribution chosen before. For that specific distribution we obtain the following bound : \n",
    "\n",
    "$$\n",
    "\\forall t>0, \\eta_t = \\frac{ln(N)}{\\sum_{s=1}^{t-1} \\delta_s} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "Other functions I used in the report, notably visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_weights_3d(matrix):\n",
    "    \"\"\"\n",
    "    Note :\n",
    "    This method is used to create 3D-graphs to plot the errors or weights accross time.\n",
    "    - matrix : an T by N matrix containing the errors or weights of the N predictors for each iteration between 1 and T.\n",
    "    \"\"\"\n",
    "    T,N = matrix.shape\n",
    "    data = []\n",
    "    for n in range(N):\n",
    "        x = np.ones(T)\n",
    "        y = np.arange(T)\n",
    "        z = matrix[:T,n]\n",
    "\n",
    "        trace = go.Scatter3d(\n",
    "        x=n*x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        mode='lines',\n",
    "            marker=dict(\n",
    "                size=12,\n",
    "                line=dict(\n",
    "                    color='rgba(217, 217, 217, 0.14)',\n",
    "                    width=0.5   \n",
    "                ),\n",
    "                opacity=0.8,\n",
    "                symbol = \"circle\",\n",
    "            ),\n",
    "\n",
    "        )\n",
    "        data.append(trace)\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title = \"weights accross time\",\n",
    "        margin=dict(l=0, r=0, b=0, t=5),\n",
    "        scene=go.Scene(\n",
    "            xaxis=go.XAxis(title='Expert index'),\n",
    "            yaxis=go.YAxis(title='Time'),\n",
    "            zaxis=go.ZAxis(title='Weight')\n",
    "        )\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return py.iplot(fig, filename='simple-3d-scatter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_heatmap(weights_matrix):\n",
    "    data = [\n",
    "        go.Heatmap(\n",
    "            z=weights_matrix,\n",
    "            colorscale='Viridis'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title='Evolution of the weights accross time',\n",
    "        xaxis = dict(title = 'Expert index',),\n",
    "        yaxis = dict(title = 'Number of iterations',)\n",
    "    )\n",
    "    figure = go.Figure(data=data, layout=layout)\n",
    "    return py.iplot(figure, filename='weights-heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
