{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential Learning Project : sequential weighted scheme comparison\n",
    "_Supervisor : Gilles Stoltz (http://stoltz.perso.math.cnrs.fr/enseignements.html)_\n",
    "\n",
    "_Author : Hugo Vallet (https://uk.linkedin.com/in/hugovallet)_\n",
    "\n",
    "_Course : Learning and sequential optimization, Data Science track, Paris-Saclay University_\n",
    "\n",
    "## Intoduction\n",
    "In this report, we will test multiple weighting schemes for sequential learning problems. As a reminder, the sequential learning setup is the following :\n",
    "- We have a set of $N$ \"experts\", or predictors, $N \\in \\mathbb{N}$\n",
    "- We get the data \"online\", meaning that we do not have access directly to a batch : the examples (data points in $\\mathbb{R}^d$) are coming iteratively in time.\n",
    "- At each step $t$, for a given data point $x_t$, the prediction is a weighted sum of the individual predictions of the  experts. \n",
    "\n",
    "The main challenge here is to find the best choice of weights at a time t, given the data (and the associated errors of the experts) seen in the past.\n",
    "\n",
    "## Dataset\n",
    "Here, I consider first a toy regression problem generated using the library Sklearn. This dataset is composed of 10000 iid obeservations in $\\mathbb{R}^{15}$ following a Gaussian distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "py.sign_in('hugovallet', '1po3tzu1j2')\n",
    "\n",
    "#Generate the dataset\n",
    "data,y = make_regression(n_samples = 10000, n_features = 15, n_informative = 15, n_targets = 1 , bias = 0, noise=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of experts and methodology\n",
    "The aim of the report is to study the behaviour of the Ridge-weighting and EWA algorithms on a given set of predictors for an incoming flow of data. Hence, we are not really interested in optimizing the choice of experts. Finally, these experts are fixed at the beginning of the problem : they are not intrinsectly modified during the online process, only their weights are updated.\n",
    "\n",
    "With this setup, I created a set of random trees that I will use as experts. The trees are generated using a random-forest training approach :\n",
    "\n",
    "For each tree :\n",
    "- Select randomly a set of features on which the tree will be trained\n",
    "- Construct the tree splitting iteratively on a random subset of this features. The split is chosen with a entropy-based criterion : the chosen split is the one maximizing the \"information\" gain.\n",
    "\n",
    "I use half of my data for this initial training of the experts and I keep the remaining half for the study of the online weighting algorithms (Ridge and EWA). This approach seems valid in the sens that :\n",
    "- The datapoints follow the same distribution. Thus, theorically, the experts learnt during the first phase are relevant for the sequential problem.\n",
    "- Because of the random forest procedure (decribed above) the trees generated are really of variable efficiency. Some of them, which are based on very informative features, perform a lot better than others. Thus, finding a good way to average their predictions (and not taking just the basic average, as used in classical random forest prediction) is relevant.\n",
    "\n",
    "### First phase : training of the experts (generating the trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted R-squared score :  0.570444315127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=42)\n",
    "reg = ExtraTreesRegressor(n_estimators=50, max_features='sqrt')\n",
    "reg.fit(X_train,y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(\"Adjusted R-squared score : \",r2_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using random forests, it is always interesting to check the features which contribute the most is the model. Sklearn implementation proposes a keys-in-hand method to compute an individual score of the different features which is directly linked to the number of times the features where selected for a split (i.e. how informative they are)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/6.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances = reg.feature_importances_\n",
    "trace = go.Bar(\n",
    "         x=[(\"Feature\"+str(i)) for i in range(len(feature_importances))],\n",
    "         y=feature_importances\n",
    ")\n",
    "layout = go.Layout(\n",
    "        title='Features importances',\n",
    "        yaxis = dict(title = 'Importance measure',)\n",
    "    )\n",
    "trace = [trace]\n",
    "figure = go.Figure(data=trace, layout=layout)\n",
    "py.iplot(figure, filename='importances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this graph, it is easy to intuit that the trees which are not based on the features 5 and 14, for instance, will be less efficient than the other ones. To check that, let's plot the individual errors of our trees in the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/42.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "trees = reg.estimators_\n",
    "errors = []\n",
    "for tree in trees :\n",
    "    error = mean_squared_error(y_test,tree.predict(X_test))\n",
    "    errors.append(error)\n",
    "    \n",
    "trace = go.Bar(\n",
    "         x=np.arange(len(errors)),\n",
    "         y=errors\n",
    ")\n",
    "layout = go.Layout(\n",
    "        title='MSE per expert on test set',\n",
    "        xaxis = dict(title = 'Expert index',),\n",
    "        yaxis = dict(title = 'MSE',)\n",
    "    )\n",
    "trace = [trace]\n",
    "figure = go.Figure(data=trace, layout=layout)\n",
    "py.iplot(figure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the real value of series to predict and the prediction made by a uniform average of the predictions of the trees "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/8.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trace1 = go.Scatter(x = np.arange(len(y_test)),y = y_test,name = \"Real value\")\n",
    "trace2 = go.Scatter(x = np.arange(len(y_pred)),y = y_pred, name = \"Predicted value (uniform average of predictors)\")\n",
    "trace = [trace1, trace2]\n",
    "py.iplot(trace, filename='prediction VS real value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of the sequential algorithms\n",
    "\n",
    "Here are implemented to classical sequential agolrithms : EWA and Riddge.\n",
    "\n",
    "### EWA (Exponentially Weighted Average)\n",
    "\n",
    "In the EWA algorithm, the weights are updated respecting the following law :\n",
    "\n",
    "$$ \\forall j\\in[1,N], w_{1,j} = \\frac{1}{N} $$\n",
    "\n",
    "$$ \\forall(t,j)\\in [2,T]\\times[1,N],  w_{t,j} = \\frac{1}{W}exp(-\\eta\\sum\\limits_{s=1}^{t-1} l_{s,j}) $$\n",
    "\n",
    "whith $W$ a normalization factor (to make the weights sum to 1) and $l_{s,j}$ the error (loss) observed for the predictor $j$ at the step $s$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_EWA_weights(eta,weights_matrix,loss_matrix):\n",
    "    \"\"\"\n",
    "    This method takes a weights and loss matrices as input (size t,N where t is the current number of iterations) \n",
    "    and returns the new row as well as the updated weights matrix for further computations.\n",
    "    \"\"\"\n",
    "    weights = np.exp(-eta*np.sum(loss_matrix,axis=0))\n",
    "    W = np.sum(weights)\n",
    "    weights = np.expand_dims(weights/W,axis=0)\n",
    "    \n",
    "    return weights\n",
    "    \n",
    "def compute_losses(experts, X, y, loss = \"least squares\"):\n",
    "    \"\"\"\n",
    "    This methods takes as input the loss function we want to use to measure the distance between the predictions of \n",
    "    the experts, for a given example X (vector of observations), and the real value y.\n",
    "    The experts here are all sklearn regressors objects stocked in an array.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    for expert in experts :\n",
    "        pred = expert.predict(X)[0]\n",
    "        if loss == \"least squares\" :\n",
    "            obs_loss = (y-pred)**2\n",
    "            losses.append(obs_loss)\n",
    "    \n",
    "    \n",
    "    return losses\n",
    "\n",
    "def weighted_prediction(experts,xt,weights):\n",
    "    predictions = []\n",
    "    for expert in experts :\n",
    "        pred = expert.predict(xt)[0]\n",
    "        predictions.append(pred)\n",
    "    predictions = np.array(predictions)\n",
    "    return np.sum(weights*predictions)\n",
    "    \n",
    "def draw_example(X,y):\n",
    "    n_examples = X.shape[0]\n",
    "    index = np.random.randint(low=0,high=n_examples)\n",
    "    Xt = X[index,:].reshape(1, -1)\n",
    "    yt = y[index]\n",
    "    return Xt,yt\n",
    "\n",
    "def compute_regret(weights_matrix,loss_matrix):\n",
    "    \"\"\"\n",
    "    Compute the regret given the 2 matrices of interest.\n",
    "    \"\"\"\n",
    "    #Compute the term on the left of the regret formula for a linear loss function \n",
    "    first_term = sum(sum(weights_matrix*loss_matrix))\n",
    "    second_term = min(np.sum(loss_matrix,axis=0))\n",
    "    return first_term - second_term\n",
    "\n",
    "def compute_bound(N,eta,time,m,M):\n",
    "    return np.log(N)/eta + eta*(M-m)**2/8*time   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, we have a batch of data. To simulate the online approach, at each step t in the algorithm, we will draw uniformly an example on that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def EWA(experts, X, y, T, eta):\n",
    "    #Initialization\n",
    "    N = len(experts)\n",
    "    \n",
    "    weights_matrix = np.zeros((T,N))\n",
    "    weights_matrix[0,:] = 1/float(N)*np.ones((1,N))\n",
    "    weights = weights_matrix[0,:]\n",
    "    loss_matrix = np.zeros((T,N))\n",
    "    #error vec 1 stocks the errors made by the aggregated model\n",
    "    error_vec1=[]\n",
    "    #error vec 2 stocks the error made by the random forest (for further performances comparisons)\n",
    "    error_vec2=[]\n",
    "    #The regrets at each step for our algorithm\n",
    "    regrets = []\n",
    "    \n",
    "    #Start sequential learning\n",
    "    for t in range(T):\n",
    "        #Draw an example from the batch\n",
    "        Xt,yt = draw_example(X,y)\n",
    "        #Oberve the losses of the experts\n",
    "        loss_matrix[t,:] = compute_losses(experts,Xt,yt,loss = \"least squares\")\n",
    "        #Make a prediction\n",
    "        y_pred = weighted_prediction(experts,Xt,weights)\n",
    "        #Compute regret    \n",
    "        regret = compute_regret(weights_matrix,loss_matrix)\n",
    "        regrets.append(regret)\n",
    "        #Update weights\n",
    "        if t<T-1:\n",
    "            weights_matrix[t+1,:] = compute_EWA_weights(eta,weights_matrix,loss_matrix)\n",
    "            weights = weights_matrix[t+1,:]\n",
    "            \n",
    "        #Stock errors for plotting\n",
    "        error1 = (yt-y_pred)**2\n",
    "        error_vec1.append(error1)\n",
    "        error2 = (yt-reg.predict(Xt)[0])**2\n",
    "        error_vec2.append(error2)\n",
    "    \n",
    "    return loss_matrix, weights_matrix, error_vec1,error_vec2,regrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X_test\n",
    "y = y_test\n",
    "T=2000\n",
    "experts = reg.estimators_\n",
    "eta = 0.00000001\n",
    "loss_matrix, weights_matrix, error_vec1,error_vec2,regrets = EWA(experts, X, y, T, eta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The draw time for this plot will be slow for all clients.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WinPython-64bit-3.4.4.1\\python-3.4.4.amd64\\lib\\site-packages\\plotly\\plotly\\plotly.py:1416: UserWarning:\n",
      "\n",
      "Estimated Draw Time Too Long\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_weights_3d(weights_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the high number of experts, this plot is a bit messy, here is another visualization of the evolution of weights accross time :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_heatmap(weights_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is clear that choosing a good $\\eta$ is foundamental : a too large value will make the update rules too sensitive and thus block the weights on few experts, a too small value will restrain the algorithm's ability to adapt to the incoming flow of data and the prediction will be similar to a classical RF.\n",
    "\n",
    "In the previous example I cheated : because I have a batch of data, I computed a first time the loss matrix to have an idea of $m$ and $M$ so that i could take an \"optimal\" fixed $\\eta$. It's interesting to notice that, after 2000 iterations, the values are not very different from the mean (0.2) but there still some experts that are distinguished and higher weighted than others.\n",
    "\n",
    "### EWA VS classic random forest\n",
    "\n",
    "After the previous observation, it is interesting to check if, in reality, the EWA outperforms the random forest (remember, with the experts I chose, the random forest is just the EWA with fixed in time equal weights for all experts !)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/26.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time = np.arange(T)\n",
    "\n",
    "plot_error_vecs(error_vec1,error_vec2,time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that EWA is slightly beating the RF, on average. However, the improvement remains, in that case, pretty small. I explain this by two factors \n",
    "- I chose a large colection of experts (50), in practice aggregation deals with more restrained collections of experts (from what I've seen in different papers, for instance : ftp://ftp.dma.ens.fr/pub/reports/dma-07-08.pdf). With a high number of experts, the individual predictions will be diluted in the overall score.\n",
    "- The experts I chose don't have a lot of variance : they are all trees, making predictions on the same criterions. Aggregation excels in the situations where we have a lot of difference between the experts.\n",
    "\n",
    "### Influence of the number of experts\n",
    "\n",
    "After the previous comclusion I wanted to see if reducing the number of experts could increase the difference in the RF and EWA repsective predictive power. To do that, I trained a \"bad\" random forest : a forest with only 5 trees of different accuracy and then bagged them using EWA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/28.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg2 = ExtraTreesRegressor(n_estimators=5, max_features='sqrt')\n",
    "reg2.fit(X_train,y_train)\n",
    "experts2 = reg2.estimators_\n",
    "loss_matrix, weights_matrix, error_vec3,error_vec4,regrets = EWA(experts2, X, y, T, eta)\n",
    "plot_error_vecs(error_vec3,error_vec4,time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_heatmap(weights_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variable step EWA\n",
    "\n",
    "To have a better control on the regret, a classical approach is to make $\\eta$ vary accross time. \n",
    "One possible implementation is to make the $\\eta$ depending on the distribution chosen before. For that specific distribution we obtain the following bound : \n",
    "\n",
    "$$\n",
    "\\forall t>0, \\eta_t = \\frac{ln(N)}{\\sum_{s=1}^{t-1} \\delta_s} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Ridge aggregation \n",
    "\n",
    "In this part we will implement and compare Ridge aggregation with the EWA.\n",
    "The main difference with EWA is, basically, the choice of the weights. \n",
    "\n",
    "At each step, they are determined solving the well-known Ridge minimization problem :\n",
    "$$\n",
    "w_0 = (\\frac{1}{N},...,\\frac{1}{N})\n",
    "$$\n",
    "$$ \n",
    "\\forall t>0,w_t = argmin_{w \\in \\mathbb{R}^N} \\{  \\lambda||w||^2_2 + \\sum_{s=1}^{t-1}(\\sum_{j}w_j f_{j,s} - y_s  )^2 \\}\n",
    "$$\n",
    "\n",
    "The lazy way of solving that problem would be to call iteratively a solver like cvxopt. Instead, I prefer to use the update rules proposed by Gerchinovitz & Gaillard :\n",
    "We define :\n",
    "$$\n",
    "A_t = \\lambda I_N + \\sum_{s=1}^{t}f_{s}f^T_{s}\n",
    "$$\n",
    "With $f_s $ the column vector of the experts' predictions at step $s$. Starting with $A_0 = \\lambda I_N$ the update rules for the weights and A are the following :\n",
    "$$\n",
    "A_{s+1} = A_s + f_{t+1}f^T_{t+1}\n",
    "$$\n",
    "$$\n",
    "w_{s+1} = w_s - A^{-1}_s(f^T_s w_s - y_s)f_s\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_predictions(experts, xs):\n",
    "    fs = []\n",
    "    N = len(y)\n",
    "    for expert in experts :\n",
    "        pred = expert.predict(xs)[0]\n",
    "        fs.append(pred)\n",
    "        \n",
    "    fs = np.array(fs)\n",
    "    fs = np.expand_dims(fs,axis=1) #Create a column vector for further matrices proucts\n",
    "    return fs\n",
    "\n",
    "def update_A(A,fs):    \n",
    "    return A + np.dot(fs,fs.T)\n",
    "\n",
    "def update_ridge_weights(w,A,fs,ys):\n",
    "    A_inv = np.linalg.inv(A)\n",
    "    temp = (np.dot(fs.T,w)-ys)[0,0] #This quantity is a scalar !\n",
    "    w = w - temp*np.dot(A_inv, fs)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ridge(experts, T, lamb):\n",
    "    #Initialization\n",
    "    N = len(experts)\n",
    "    A = lamb * np.eye(N)\n",
    "    \n",
    "    weights_matrix = np.zeros((T,N))\n",
    "    weights_matrix[0,:] = 1/float(N)*np.ones((1,N))\n",
    "    weights = weights_matrix[0,:]\n",
    "    weights = np.expand_dims(weights,axis=1)\n",
    "    loss_matrix = np.zeros((T,N))\n",
    "    \n",
    "    #error vec 1 stocks the errors made by the aggregated model\n",
    "    error_vec1=[]\n",
    "    #error vec 2 stocks the error made by the random forest (for further performances comparisons)\n",
    "    error_vec2=[]\n",
    "    \n",
    "    #Entering the sequential loop\n",
    "    for t in range(T):\n",
    "        #Draw an example from the batch\n",
    "        Xt,yt = draw_example(X,y)\n",
    "        #Compute individual predictions\n",
    "        ft = compute_predictions(experts, Xt)\n",
    "        \n",
    "        #Make weighted prediction\n",
    "        y_pred = np.dot(ft.T,weights)\n",
    "        #Update matrix A\n",
    "        A = update_A(A,ft)\n",
    "        #Update current weights and weights matrix\n",
    "        if t<T-1 :\n",
    "            weights = update_ridge_weights(weights,A,ft,yt)\n",
    "            weights_matrix[t+1,:] = weights[:,0]\n",
    "            \n",
    "        #Stock errors for plotting\n",
    "        error1 = (yt-y_pred[0,0])**2\n",
    "        error_vec1.append(error1)\n",
    "        error2 = (yt-reg.predict(Xt)[0])**2\n",
    "        error_vec2.append(error2)\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    return weights_matrix, error_vec1, error_vec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "T = 2000\n",
    "lamb = 0.0001\n",
    "ridge_weight, ridge_error_vec1, ridge_error_vec2 = ridge(experts2, T, lamb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/10.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_heatmap(ridge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/2.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_weights_3d(ridge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~hugovallet/40.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_error_vecs(ridge_error_vec1,ridge_error_vec2,time,\"RIDGE\", \"RF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ridge seems to be a lot faster to converge to a stable weights distribution than EWA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix\n",
    "\n",
    "Other functions I used in the report, notably visualization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_weights_3d(matrix):\n",
    "    \"\"\"\n",
    "    Note :\n",
    "    This method is used to create 3D-graphs to plot the errors or weights accross time.\n",
    "    - matrix : an T by N matrix containing the errors or weights of the N predictors for each iteration between 1 and T.\n",
    "    \"\"\"\n",
    "    T,N = matrix.shape\n",
    "    data = []\n",
    "    for n in range(N):\n",
    "        x = np.ones(T)\n",
    "        y = np.arange(T)\n",
    "        z = matrix[:T,n]\n",
    "\n",
    "        trace = go.Scatter3d(\n",
    "        x=n*x,\n",
    "        y=y,\n",
    "        z=z,\n",
    "        mode='lines',\n",
    "            marker=dict(\n",
    "                size=12,\n",
    "                line=dict(\n",
    "                    color='rgba(217, 217, 217, 0.14)',\n",
    "                    width=0.5   \n",
    "                ),\n",
    "                opacity=0.8,\n",
    "                symbol = \"circle\",\n",
    "            ),\n",
    "\n",
    "        )\n",
    "        data.append(trace)\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title = \"weights accross time\",\n",
    "        margin=dict(l=0, r=0, b=0, t=5),\n",
    "        scene=go.Scene(\n",
    "            xaxis=go.XAxis(title='Expert index'),\n",
    "            yaxis=go.YAxis(title='Time'),\n",
    "            zaxis=go.ZAxis(title='Weight')\n",
    "        )\n",
    "    )\n",
    "    fig = go.Figure(data=data, layout=layout)\n",
    "    return py.iplot(fig, filename='simple-3d-scatter')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_heatmap(weights_matrix):\n",
    "    data = [\n",
    "        go.Heatmap(\n",
    "            z=weights_matrix,\n",
    "            colorscale='Viridis'\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    layout = go.Layout(\n",
    "        title='Evolution of the weights accross time',\n",
    "        xaxis = dict(title = 'Expert index',),\n",
    "        yaxis = dict(title = 'Number of iterations',)\n",
    "    )\n",
    "    figure = go.Figure(data=data, layout=layout)\n",
    "    return py.iplot(figure, filename='weights-heatmap')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_error_vecs(error_vec1,error_vec2,time, label1, label2):\n",
    "    trace1 = go.Scatter(\n",
    "    x = time,\n",
    "    y = error_vec1,\n",
    "    opacity = 0.75,\n",
    "    name = 'Residuals '+label1,\n",
    "    line = dict(\n",
    "        color = ('rgb(205, 12, 24)'),\n",
    "        width = 1)\n",
    "    )\n",
    "    trace2 = go.Scatter(\n",
    "        x = time,\n",
    "        y = error_vec2,\n",
    "        opacity = 0.75,\n",
    "        name = 'Residuals '+label2,\n",
    "        line = dict(\n",
    "            color = ('rgb(22, 96, 167)'),\n",
    "            width = 1)\n",
    "    )\n",
    "    annotations = []\n",
    "    annotations.append(dict(xref='paper', x=0.1, y=80000,\n",
    "                                  xanchor='left', yanchor='middle',\n",
    "                                  text=\"Mean squared error \" + label1 + \" : \" + str(np.mean(error_vec1)),\n",
    "                                  font=dict(family='Arial',size=16,color='rgb(205, 12, 24)'),\n",
    "                                  showarrow=False,))\n",
    "\n",
    "    annotations.append(dict(xref='paper', x=0.1, y=75000,\n",
    "                                  xanchor='left', yanchor='middle',\n",
    "                                  text=\"Mean squared error \" + label2 + \" : \" +  str(np.mean(error_vec2)),\n",
    "                                  font=dict(family='Arial',size=16,color='rgb(22, 96, 167)'),\n",
    "                                  showarrow=False,))\n",
    "\n",
    "    layout = go.Layout()\n",
    "    traces = [trace1, trace2]\n",
    "    layout['annotations'] = annotations\n",
    "    fig = go.Figure(data=traces, layout=layout)\n",
    "    return py.iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
